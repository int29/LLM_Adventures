{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76c0f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "from tqdm.notebook import tqdm  # 노트북 환경에서 진행률 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af67ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"maywell/korean_textbooks\",data_files=\"mmlu_high_school_statistics/train-00000-of-00001.parquet\")\n",
    "ds_train = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9172bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Hugging Face 데이터셋을 스트리밍 방식으로 처리하는 PyTorch Dataset 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, max_length=1024, stride=512, column_name='0', tokenizer=\"gpt-4o\"):\n",
    "        \n",
    "        self.tokenizer = tiktoken.encoding_for_model(tokenizer)         # tiktoken 토크나이저 초기화\n",
    "        self.dataset = dataset                                          # Hugging Face 데이터셋 저장\n",
    "        self.max_length = max_length                                    # 최대 시퀀스 길이 저장\n",
    "        self.stride = stride                                            # 슬라이딩 윈도우의 보폭(stride) 저장\n",
    "        self.column_name = column_name                                  # 텍스트 데이터가 포함된 컬럼 이름 저장\n",
    "        \n",
    "        # 캐시 설정\n",
    "        self.cache = {}                                                 # 간단한 캐시 딕셔너리\n",
    "        self.cache_size = 100                                           # 8GB VRAM, 16GB RAM 환경에 맞게 설정\n",
    "        self.doc_count = len(dataset)                                   # 전체 문서 수 및 인덱스 맵핑\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 첫 번째 문서를 기반으로 평균 청크 수 추정(Loader의 batch추정 및 진행률 위해서 __len__ 필요)\n",
    "        if self.doc_count > 0:\n",
    "            # 샘플 문서 토큰화\n",
    "            sample_doc = self.dataset[0][self.column_name]\n",
    "            sample_tokens = self.tokenizer.encode(sample_doc)\n",
    "            \n",
    "            # 가능한 청크 수 계산\n",
    "            sample_chunks = max(0, (len(sample_tokens) - self.max_length) // self.stride + 1)\n",
    "            \n",
    "            # 전체 청크 수 추정\n",
    "            return max(self.doc_count, sample_chunks * self.doc_count)\n",
    "        \n",
    "        return self.doc_count  # 기본값으로 문서 수 반환\n",
    "    \n",
    "    def _process_document(self, doc_idx):\n",
    "        \"\"\"\n",
    "        문서를 처리하여 입력 및 타겟 청크 목록을 반환합니다.\n",
    "        # token_ids = [15293, 18392, 29843, 38291, 12893, 59302, 71839, 92004]\n",
    "        # max_length = 5일 때 아래와 같이 첫번째로 데이터를 처리함\n",
    "        # input_chunk = [15293, 18392, 29843, 38291, 12893]\n",
    "        # target_chunk = [18392, 29843, 38291, 12893, 59302]\n",
    "        \n",
    "        Stride가 2일 때 아래와 같이 두번째로 데이터를 처리함\n",
    "        # input_chunk = [29843, 38291, 12893, 59302, 71839]\n",
    "        # target_chunk = [38291, 12893, 59302, 71839, 92004]\n",
    "        \n",
    "        # input_chunk와 target_chunk 쌍을 비교해보면 target_chunk는 input_chunk보다 한 토큰 앞으로 이동한 것임을 알 수 있습니다.\n",
    "        # 즉, max_length만큼 데이터가 증분됨.\n",
    "        input[0] = 15293   -->   target[0] = 18392 (= input[1])\n",
    "        input[1] = 18392   -->   target[1] = 29843 (= input[2])\n",
    "        input[2] = 29843   -->   target[2] = 38291 (= input[3])\n",
    "        ...\n",
    "        \"\"\"\n",
    "        # 문서 텍스트 가져오기\n",
    "        doc_text = self.dataset[doc_idx][self.column_name]\n",
    "        \n",
    "        # tiktoken으로 토큰화\n",
    "        token_ids = self.tokenizer.encode(doc_text)\n",
    "        \n",
    "        input_chunks = []\n",
    "        target_chunks = []\n",
    "        \n",
    "        # 슬라이딩 윈도우를 사용하여 토큰 ID 리스트를 청크로 나눔\n",
    "        for i in range(0, len(token_ids) - self.max_length, self.stride):\n",
    "            # 입력 청크: 인덱스 i부터 시작하는 max_length 길이의 토큰 시퀀스\n",
    "            input_chunk = token_ids[i : i + self.max_length]\n",
    "            \n",
    "            # 타겟 청크: 입력보다 한 토큰 뒤에서 시작 (자동회귀 예측을 위한 1토큰 증분)\n",
    "            # 예: 입력=[1,2,3,4], 타겟=[2,3,4,5]\n",
    "            target_chunk = token_ids[i + 1 : i + self.max_length + 1]\n",
    "            \n",
    "            # 입력 청크와 타겟 청크의 길이가 모두 max_length인지 확인\n",
    "            if len(input_chunk) == self.max_length and len(target_chunk) == self.max_length:\n",
    "                # 텐서로 변환하여 리스트에 추가\n",
    "                input_chunks.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "                target_chunks.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "        \n",
    "        return input_chunks, target_chunks\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        인덱스에 해당하는 데이터 항목을 반환합니다.\n",
    "        스트리밍 방식으로 필요할 때 문서를 처리합니다.\n",
    "        \"\"\"\n",
    "        # 캐시 확인\n",
    "        # 이미 처리한 인덱스는 캐시에 저장해두고, 다시 요청이 들어오면 재계산 없이 바로 반환합니다.\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # 문서 인덱스 계산(입력된 idx를 문서 총 개수(self.doc_count)로 나눈 나머지를 사용합니다.)\n",
    "        # 예시: idx=15, 문서 수가 10개라면, doc_idx=5가 됩니다.\n",
    "        # 이렇게 하면 idx가 문서 수보다 커져도 항상 유효한 문서 인덱스를 얻을 수 있습니다.\n",
    "        doc_idx = idx % self.doc_count\n",
    "        \n",
    "        \"\"\"\n",
    "        [참고]\n",
    "        유한한 문서 집합에서 무한한 인덱스 처리:\n",
    "\n",
    "        데이터셋에는 유한한 수의 문서(예: 1,000개)가 있습니다.\n",
    "        하지만 __getitem__은 이론적으로 매우 큰 인덱스(예: 5,000)를 요청받을 수 있습니다.\n",
    "        이때 idx % self.doc_count 연산으로 실제 사용 가능한 문서 인덱스로 변환합니다.\n",
    "\n",
    "\n",
    "        순환 접근 패턴:\n",
    "\n",
    "        이 방식은 문서들을 계속해서 순환하며 접근할 수 있게 합니다.\n",
    "        예: 문서가 3개(0, 1, 2)일 때, idx가 3이면 다시 문서 0으로, 4면 문서 1로 돌아갑니다.\n",
    "\n",
    "\n",
    "        예시로 설명:\n",
    "\n",
    "        문서가 5개 있다고 가정합시다(인덱스 0~4).\n",
    "        idx = 7이 들어오면, 7 % 5 = 2가 되어 세 번째 문서(인덱스 2)에 접근합니다.\n",
    "        idx = 12가 들어오면, 12 % 5 = 2가 되어 동일하게 세 번째 문서에 접근합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        \n",
    "        # 문서 처리(고정슬라이드드)\n",
    "\n",
    "        input_chunks, target_chunks = self._process_document(doc_idx)\n",
    "        \n",
    "        # 처리된 청크가 없으면 다음 문서 시도\n",
    "        if not input_chunks:\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        # 청크 인덱스 계산\n",
    "        # 청크로 문서를 나누기 때문에 마찬가지로 필요\n",
    "        \n",
    "        chunk_idx = (idx // self.doc_count) % max(1, len(input_chunks))\n",
    "        \"\"\"\n",
    "        [참고]\n",
    "        문서가 3개 있고, 각 문서에서 생성된 청크 수가 다음과 같다고 가정해봅시다:\n",
    "\n",
    "        문서 0: 4개 청크\n",
    "        문서 1: 2개 청크\n",
    "        문서 2: 3개 청크\n",
    "\n",
    "        chunk사이즈가 5라면, idx가 0~4일 때는 문서 0의 청크를 가져오고,\n",
    "        idx가 5~9일 때는 문서 1의 청크를 가져오고, idx가 10~14일 때는 문서 2의 청크를 가져옵니다. \n",
    "        3번이 순환이 되면서 15번째 idx를 요구한다면면 자동으로 문서의 크기로 나눠 주기 때문에 index오류를 방지할 수 있습니다.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # 결과 저장\n",
    "        result = (input_chunks[chunk_idx % len(input_chunks)], target_chunks[chunk_idx % len(input_chunks)])\n",
    "        \n",
    "        # 캐시 업데이트 (제한된 크기)\n",
    "        if len(self.cache) < self.cache_size:\n",
    "            self.cache[idx] = result\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60517e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터셋 정보 ===\n",
      "데이터셋 크기: 76033 개의 문서\n",
      "데이터셋 컬럼: ['0']\n",
      "\n",
      "=== 첫 번째 문서 샘플 ===\n",
      "문서 내용 (처음 300자): \"도서관 vs. 책 가게, 어느 것이 좀 더 중요한가?\"에 대한 토론 내용:\n",
      "\n",
      "**Phi:** 안녕, Epsilon. 오늘은 도서관과 서점에 대해 이야기하고 싶다.\n",
      "\n",
      "**Epsilon:** 네, Phi. 무슨 이야기인지 궁금하다.\n",
      "\n",
      "**Phi:** 도서관과 책 가게는 모두 책을 제공한다는 점에서 비슷하다. 하지만 두 가지 장소는 서로 다른 점도 많다. 도서관은 공공 기관이며, 책을 무료로 대출할 수 있다. 반면에 서점은 사설 기업이며, 책을 판매한다.\n",
      "\n",
      "**Epsilon:** 네, 그렇다. 그리고 도서관은 일반적으로 책의 종류가 더...\n"
     ]
    }
   ],
   "source": [
    "print(\"=== 데이터셋 정보 ===\")\n",
    "print(f\"데이터셋 크기: {len(ds_train)} 개의 문서\")\n",
    "print(f\"데이터셋 컬럼: {ds_train.column_names}\")\n",
    "print(\"\\n=== 첫 번째 문서 샘플 ===\")\n",
    "first_doc = ds_train[0]\n",
    "column_name = '0'\n",
    "print(f\"문서 내용 (처음 300자): {first_doc[column_name][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f34d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: \"도서관 vs. 책 가게, 어느 것이 좀 더 중요한가?\"에 대한 토론 내용:\n",
      "\n",
      "**Phi:** 안녕, Epsilon. 오늘은 도서관과 서점에 대해 이야기하고 싶다.\n",
      "\n",
      "**Epsil\n",
      "토큰 개수: 51\n",
      "토큰 ID: [1, 5827, 4865, 15853, 10217, 13, 67671, 9919, 7996, 11, 138744, 46947, 107894, 28526, 141410, 4081, 16842, 3107, 33398, 68258]...\n",
      "디코딩된 토큰: ['\"', '도', '서', '관', ' vs', '.', ' 책', ' 가', '게', ',', ' 어느', ' 것이', ' 좀', ' 더', ' 중요한', '가', '?\"', '에', ' 대한', ' 토']...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "sample_text = first_doc[column_name][:100]  # 문서의 처음 100자\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in tokens]\n",
    "print(f\"원문: {sample_text}\")\n",
    "print(f\"토큰 개수: {len(tokens)}\")\n",
    "print(f\"토큰 ID: {tokens[:20]}...\")\n",
    "print(f\"디코딩된 토큰: {decoded_tokens[:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c062799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CustomDataset 정보 ===\n",
      "총 추정 청크 수: 9884290\n"
     ]
    }
   ],
   "source": [
    "# 3. CustomDataset 객체 생성 및 테스트\n",
    "# 짧은 max_length와 stride 값을 사용하여 예제 결과 확인\n",
    "max_length = 10  # 시각화를 위해 작은 값 사용\n",
    "stride = 5\n",
    "custom_ds = CustomDataset(ds_train, max_length=max_length, stride=stride, column_name=column_name,tokenizer=\"gpt-4o\")\n",
    "print(f\"\\n=== CustomDataset 정보 ===\")\n",
    "print(f\"총 추정 청크 수: {len(custom_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91566c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 슬라이딩 윈도우 처리 ===\n",
      "첫 번째 문서의 토큰 수: 659\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>시작 인덱스</th>\n",
       "      <th>입력 청크 ID</th>\n",
       "      <th>입력 청크 텍스트</th>\n",
       "      <th>타겟 청크 ID</th>\n",
       "      <th>타겟 청크 텍스트</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 5827, 4865, 15853, 102...</td>\n",
       "      <td>\"도서관 vs. 책 가게,</td>\n",
       "      <td>[5827, 4865, 15853, 10217,...</td>\n",
       "      <td>도서관 vs. 책 가게, 어느</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>[13, 67671, 9919, 7996, 11...</td>\n",
       "      <td>. 책 가게, 어느 것이 좀 더 중요한</td>\n",
       "      <td>[67671, 9919, 7996, 11, 13...</td>\n",
       "      <td>책 가게, 어느 것이 좀 더 중요한가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[138744, 46947, 107894, 28...</td>\n",
       "      <td>어느 것이 좀 더 중요한가?\"에 대한 토</td>\n",
       "      <td>[46947, 107894, 28526, 141...</td>\n",
       "      <td>것이 좀 더 중요한가?\"에 대한 토론</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>[4081, 16842, 3107, 33398,...</td>\n",
       "      <td>가?\"에 대한 토론 내용:\\n\\n**Phi</td>\n",
       "      <td>[16842, 3107, 33398, 68258...</td>\n",
       "      <td>?\"에 대한 토론 내용:\\n\\n**Phi:**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>[39168, 69634, 1402, 410, ...</td>\n",
       "      <td>론 내용:\\n\\n**Phi:** 안녕, E</td>\n",
       "      <td>[69634, 1402, 410, 72002, ...</td>\n",
       "      <td>내용:\\n\\n**Phi:** 안녕, Epsilon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   시작 인덱스                       입력 청크 ID                입력 청크 텍스트  \\\n",
       "0       0  [1, 5827, 4865, 15853, 102...           \"도서관 vs. 책 가게,   \n",
       "1       5  [13, 67671, 9919, 7996, 11...    . 책 가게, 어느 것이 좀 더 중요한   \n",
       "2      10  [138744, 46947, 107894, 28...   어느 것이 좀 더 중요한가?\"에 대한 토   \n",
       "3      15  [4081, 16842, 3107, 33398,...  가?\"에 대한 토론 내용:\\n\\n**Phi   \n",
       "4      20  [39168, 69634, 1402, 410, ...  론 내용:\\n\\n**Phi:** 안녕, E   \n",
       "\n",
       "                        타겟 청크 ID                     타겟 청크 텍스트  \n",
       "0  [5827, 4865, 15853, 10217,...              도서관 vs. 책 가게, 어느  \n",
       "1  [67671, 9919, 7996, 11, 13...          책 가게, 어느 것이 좀 더 중요한가  \n",
       "2  [46947, 107894, 28526, 141...          것이 좀 더 중요한가?\"에 대한 토론  \n",
       "3  [16842, 3107, 33398, 68258...     ?\"에 대한 토론 내용:\\n\\n**Phi:**  \n",
       "4  [69634, 1402, 410, 72002, ...   내용:\\n\\n**Phi:** 안녕, Epsilon  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# 4. 첫 번째 문서를 토큰화하고 슬라이딩 윈도우 시각화\n",
    "first_doc_text = ds_train[0][column_name]\n",
    "first_doc_tokens = tokenizer.encode(first_doc_text)\n",
    "    \n",
    "print(f\"\\n=== 슬라이딩 윈도우 처리 ===\")\n",
    "print(f\"첫 번째 문서의 토큰 수: {len(first_doc_tokens)}\")\n",
    "window_results = []\n",
    "for i in range(0, len(first_doc_tokens) - max_length, stride):\n",
    "    input_chunk = first_doc_tokens[i : i + max_length]\n",
    "    print\n",
    "    target_chunk = first_doc_tokens[i + 1 : i + max_length + 1]\n",
    "    \n",
    "    if len(input_chunk) == max_length and len(target_chunk) == max_length:\n",
    "        input_decoded = tokenizer.decode(input_chunk)\n",
    "        target_decoded = tokenizer.decode(target_chunk)\n",
    "        window_results.append({\n",
    "            \"시작 인덱스\": i,\n",
    "            \"입력 청크 ID\": input_chunk,\n",
    "            \"입력 청크 텍스트\": input_decoded,\n",
    "            \"타겟 청크 ID\": target_chunk,\n",
    "            \"타겟 청크 텍스트\": target_decoded\n",
    "        })\n",
    "#  \n",
    "results_df = pd.DataFrame(window_results)\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f779cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터셋 정보 ===\n",
      "데이터셋 크기: 76033 개의 문서\n",
      "데이터셋 컬럼: ['0']\n",
      "\n",
      "=== 첫 번째 문서 샘플 ===\n",
      "문서 내용 (처음 300자): \"도서관 vs. 책 가게, 어느 것이 좀 더 중요한가?\"에 대한 토론 내용:\n",
      "\n",
      "**Phi:** 안녕, Epsilon. 오늘은 도서관과 서점에 대해 이야기하고 싶다.\n",
      "\n",
      "**Epsilon:** 네, Phi. 무슨 이야기인지 궁금하다.\n",
      "\n",
      "**Phi:** 도서관과 책 가게는 모두 책을 제공한다는 점에서 비슷하다. 하지만 두 가지 장소는 서로 다른 점도 많다. 도서관은 공공 기관이며, 책을 무료로 대출할 수 있다. 반면에 서점은 사설 기업이며, 책을 판매한다.\n",
      "\n",
      "**Epsilon:** 네, 그렇다. 그리고 도서관은 일반적으로 책의 종류가 더...\n",
      "\n",
      "=== 토큰화 과정 ===\n",
      "원문: \"도서관 vs. 책 가게, 어느 것이 좀 더 중요한가?\"에 대한 토론 내용:\n",
      "\n",
      "**Phi:** 안녕, Epsilon. 오늘은 도서관과 서점에 대해 이야기하고 싶다.\n",
      "\n",
      "**Epsil\n",
      "토큰 개수: 51\n",
      "토큰 ID: [1, 5827, 4865, 15853, 10217, 13, 67671, 9919, 7996, 11, 138744, 46947, 107894, 28526, 141410, 4081, 16842, 3107, 33398, 68258]...\n",
      "디코딩된 토큰: ['\"', '도', '서', '관', ' vs', '.', ' 책', ' 가', '게', ',', ' 어느', ' 것이', ' 좀', ' 더', ' 중요한', '가', '?\"', '에', ' 대한', ' 토']...\n",
      "\n",
      "=== CustomDataset 정보 ===\n",
      "총 추정 청크 수: 9884290\n",
      "\n",
      "=== 슬라이딩 윈도우 처리 ===\n",
      "첫 번째 문서의 토큰 수: 659\n",
      "     시작 인덱스                       입력 청크 ID                입력 청크 텍스트  \\\n",
      "0         0  [1, 5827, 4865, 15853, 102...           \"도서관 vs. 책 가게,   \n",
      "1         5  [13, 67671, 9919, 7996, 11...    . 책 가게, 어느 것이 좀 더 중요한   \n",
      "2        10  [138744, 46947, 107894, 28...   어느 것이 좀 더 중요한가?\"에 대한 토   \n",
      "3        15  [4081, 16842, 3107, 33398,...  가?\"에 대한 토론 내용:\\n\\n**Phi   \n",
      "4        20  [39168, 69634, 1402, 410, ...  론 내용:\\n\\n**Phi:** 안녕, E   \n",
      "..      ...                            ...                      ...   \n",
      "125     625  [3281, 49647, 13070, 8963,...        을 이용할 수 있다. 반면에 서   \n",
      "126     630  [13, 35007, 9018, 3107, 24...          . 반면에 서점은 사설 기업   \n",
      "127     635  [24206, 4740, 9023, 23955,...        점은 사설 기업이며, 책을 판매   \n",
      "128     640  [128328, 11, 67671, 3281, ...     이며, 책을 판매한다. 따라서 서점을   \n",
      "129     645  [29423, 13, 171123, 24795,...     한다. 따라서 서점을 이용하려면 돈이   \n",
      "\n",
      "                          타겟 청크 ID                     타겟 청크 텍스트  \n",
      "0    [5827, 4865, 15853, 10217,...              도서관 vs. 책 가게, 어느  \n",
      "1    [67671, 9919, 7996, 11, 13...          책 가게, 어느 것이 좀 더 중요한가  \n",
      "2    [46947, 107894, 28526, 141...          것이 좀 더 중요한가?\"에 대한 토론  \n",
      "3    [16842, 3107, 33398, 68258...     ?\"에 대한 토론 내용:\\n\\n**Phi:**  \n",
      "4    [69634, 1402, 410, 72002, ...   내용:\\n\\n**Phi:** 안녕, Epsilon  \n",
      "..                             ...                           ...  \n",
      "125  [49647, 13070, 8963, 20597...              이용할 수 있다. 반면에 서점  \n",
      "126  [35007, 9018, 3107, 24795,...               반면에 서점은 사설 기업이며  \n",
      "127  [4740, 9023, 23955, 82942,...            은 사설 기업이며, 책을 판매한다  \n",
      "128  [11, 67671, 3281, 94307, 2...         , 책을 판매한다. 따라서 서점을 이용  \n",
      "129  [13, 171123, 24795, 136821...         . 따라서 서점을 이용하려면 돈이 있어  \n",
      "\n",
      "[130 rows x 5 columns]\n",
      "\n",
      "=== __getitem__ 메서드 테스트 ===\n",
      "\n",
      "항목 인덱스 0:\n",
      "문서 인덱스: 0\n",
      "입력 텐서 모양: torch.Size([10])\n",
      "타겟 텐서 모양: torch.Size([10])\n",
      "입력 텐서 (디코딩): \"도서관 vs. 책 가게,\n",
      "타겟 텐서 (디코딩): 도서관 vs. 책 가게, 어느\n",
      "\n",
      "입력-타겟 토큰 페어 비교:\n",
      "인덱스 0: 입력='\"' -> 타겟='도'\n",
      "인덱스 1: 입력='도' -> 타겟='서'\n",
      "인덱스 2: 입력='서' -> 타겟='관'\n",
      "인덱스 3: 입력='관' -> 타겟=' vs'\n",
      "인덱스 4: 입력=' vs' -> 타겟='.'\n",
      "인덱스 5: 입력='.' -> 타겟=' 책'\n",
      "인덱스 6: 입력=' 책' -> 타겟=' 가'\n",
      "인덱스 7: 입력=' 가' -> 타겟='게'\n",
      "인덱스 8: 입력='게' -> 타겟=','\n",
      "인덱스 9: 입력=',' -> 타겟=' 어느'\n",
      "\n",
      "항목 인덱스 1:\n",
      "문서 인덱스: 1\n",
      "입력 텐서 모양: torch.Size([10])\n",
      "타겟 텐서 모양: torch.Size([10])\n",
      "입력 텐서 (디코딩): \"학생의 복장 규정은 전반\n",
      "타겟 텐서 (디코딩): 학생의 복장 규정은 전반적인\n",
      "\n",
      "입력-타겟 토큰 페어 비교:\n",
      "인덱스 0: 입력='\"' -> 타겟='학생'\n",
      "인덱스 1: 입력='학생' -> 타겟='의'\n",
      "인덱스 2: 입력='의' -> 타겟=' 복'\n",
      "인덱스 3: 입력=' 복' -> 타겟='장'\n",
      "인덱스 4: 입력='장' -> 타겟=' 규'\n",
      "인덱스 5: 입력=' 규' -> 타겟='정'\n",
      "인덱스 6: 입력='정' -> 타겟='은'\n",
      "인덱스 7: 입력='은' -> 타겟=' 전'\n",
      "인덱스 8: 입력=' 전' -> 타겟='반'\n",
      "인덱스 9: 입력='반' -> 타겟='적인'\n",
      "\n",
      "항목 인덱스 2:\n",
      "문서 인덱스: 2\n",
      "입력 텐서 모양: torch.Size([10])\n",
      "타겟 텐서 모양: torch.Size([10])\n",
      "입력 텐서 (디코딩): \"오직 가난한 사람에게만 교육\n",
      "타겟 텐서 (디코딩): 오직 가난한 사람에게만 교육이\n",
      "\n",
      "입력-타겟 토큰 페어 비교:\n",
      "인덱스 0: 입력='\"' -> 타겟='오'\n",
      "인덱스 1: 입력='오' -> 타겟='직'\n",
      "인덱스 2: 입력='직' -> 타겟=' 가'\n",
      "인덱스 3: 입력=' 가' -> 타겟='난'\n",
      "인덱스 4: 입력='난' -> 타겟='한'\n",
      "인덱스 5: 입력='한' -> 타겟=' 사람'\n",
      "인덱스 6: 입력=' 사람' -> 타겟='에게'\n",
      "인덱스 7: 입력='에게' -> 타겟='만'\n",
      "인덱스 8: 입력='만' -> 타겟=' 교육'\n",
      "인덱스 9: 입력=' 교육' -> 타겟='이'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 122\u001b[0m\n\u001b[0;32m    119\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# 처리 결과 시각화 실행\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m visualize_dataset_processing()\n",
      "Cell \u001b[1;32mIn[56], line 89\u001b[0m, in \u001b[0;36mvisualize_dataset_processing\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# 6. 슬라이딩 윈도우 시각화\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first_doc_tokens) \u001b[38;5;241m>\u001b[39m max_length:\n\u001b[1;32m---> 89\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     90\u001b[0m     tokens_to_show \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;28mlen\u001b[39m(first_doc_tokens))  \u001b[38;5;66;03m# 처음 30개 토큰만 표시\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# 토큰값 표시\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 실제 처리 결과 확인을 위한 코드\n",
    "def visualize_dataset_processing():\n",
    "    # 1. 데이터셋 기본 정보 출력\n",
    "    print(\"=== 데이터셋 정보 ===\")\n",
    "    print(f\"데이터셋 크기: {len(ds_train)} 개의 문서\")\n",
    "    print(f\"데이터셋 컬럼: {ds_train.column_names}\")\n",
    "    \n",
    "    # 첫 번째 문서 출력\n",
    "    print(\"\\n=== 첫 번째 문서 샘플 ===\")\n",
    "    first_doc = ds_train[0]\n",
    "    column_name = '0' if '0' in ds_train.column_names else ds_train.column_names[0]\n",
    "    print(f\"문서 내용 (처음 300자): {first_doc[column_name][:300]}...\")\n",
    "    \n",
    "    # 2. 토큰화 과정 시각화\n",
    "    print(\"\\n=== 토큰화 과정 ===\")\n",
    "    sample_text = first_doc[column_name][:100]  # 문서의 처음 100자\n",
    "    tokens = tokenizer.encode(sample_text)\n",
    "    decoded_tokens = [tokenizer.decode([token]) for token in tokens]\n",
    "    \n",
    "    print(f\"원문: {sample_text}\")\n",
    "    print(f\"토큰 개수: {len(tokens)}\")\n",
    "    print(f\"토큰 ID: {tokens[:20]}...\")\n",
    "    print(f\"디코딩된 토큰: {decoded_tokens[:20]}...\")\n",
    "    \n",
    "    # 3. CustomDataset 객체 생성 및 테스트\n",
    "    # 짧은 max_length와 stride 값을 사용하여 예제 결과 확인\n",
    "    max_length = 10  # 시각화를 위해 작은 값 사용\n",
    "    stride = 5\n",
    "    custom_ds = CustomDataset(ds_train, max_length=max_length, stride=stride, column_name=column_name)\n",
    "    \n",
    "    print(f\"\\n=== CustomDataset 정보 ===\")\n",
    "    print(f\"총 추정 청크 수: {len(custom_ds)}\")\n",
    "    \n",
    "    # 4. 첫 번째 문서를 토큰화하고 슬라이딩 윈도우 시각화\n",
    "    first_doc_text = ds_train[0][column_name]\n",
    "    first_doc_tokens = tokenizer.encode(first_doc_text)\n",
    "    \n",
    "    print(f\"\\n=== 슬라이딩 윈도우 처리 ===\")\n",
    "    print(f\"첫 번째 문서의 토큰 수: {len(first_doc_tokens)}\")\n",
    "    \n",
    "    # 처리 결과를 DataFrame으로 시각화\n",
    "    window_results = []\n",
    "    for i in range(0, len(first_doc_tokens) - max_length, stride):\n",
    "        input_chunk = first_doc_tokens[i : i + max_length]\n",
    "        target_chunk = first_doc_tokens[i + 1 : i + max_length + 1]\n",
    "        \n",
    "        if len(input_chunk) == max_length and len(target_chunk) == max_length:\n",
    "            input_decoded = tokenizer.decode(input_chunk)\n",
    "            target_decoded = tokenizer.decode(target_chunk)\n",
    "            window_results.append({\n",
    "                \"시작 인덱스\": i,\n",
    "                \"입력 청크 ID\": input_chunk,\n",
    "                \"입력 청크 텍스트\": input_decoded,\n",
    "                \"타겟 청크 ID\": target_chunk,\n",
    "                \"타겟 청크 텍스트\": target_decoded\n",
    "            })\n",
    "    \n",
    "    if window_results:\n",
    "        results_df = pd.DataFrame(window_results)\n",
    "        pd.set_option('display.max_colwidth', 30)  # 셀 내용 표시 길이 제한\n",
    "        print(results_df)\n",
    "    else:\n",
    "        print(\"슬라이딩 윈도우로 생성된 청크가 없습니다. 문서가 너무 짧거나 max_length가 너무 큽니다.\")\n",
    "    \n",
    "    # 5. __getitem__ 메서드 테스트\n",
    "    print(\"\\n=== __getitem__ 메서드 테스트 ===\")\n",
    "    for idx in range(3):  # 처음 3개 항목 테스트\n",
    "        try:\n",
    "            input_tensor, target_tensor = custom_ds[idx]\n",
    "            print(f\"\\n항목 인덱스 {idx}:\")\n",
    "            print(f\"문서 인덱스: {idx % custom_ds.doc_count}\")\n",
    "            print(f\"입력 텐서 모양: {input_tensor.shape}\")\n",
    "            print(f\"타겟 텐서 모양: {target_tensor.shape}\")\n",
    "            print(f\"입력 텐서 (디코딩): {tokenizer.decode(input_tensor.tolist())}\")\n",
    "            print(f\"타겟 텐서 (디코딩): {tokenizer.decode(target_tensor.tolist())}\")\n",
    "            \n",
    "            # 시각화: 입력 및 타겟 비교\n",
    "            input_decoded = [tokenizer.decode([t]) for t in input_tensor.tolist()]\n",
    "            target_decoded = [tokenizer.decode([t]) for t in target_tensor.tolist()]\n",
    "            \n",
    "            print(\"\\n입력-타겟 토큰 페어 비교:\")\n",
    "            for i, (inp, tar) in enumerate(zip(input_decoded, target_decoded)):\n",
    "                print(f\"인덱스 {i}: 입력='{inp}' -> 타겟='{tar}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"인덱스 {idx}에서 오류 발생: {e}\")\n",
    "    \n",
    "    # 6. 슬라이딩 윈도우 시각화\n",
    "    if len(first_doc_tokens) > max_length:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        tokens_to_show = min(30, len(first_doc_tokens))  # 처음 30개 토큰만 표시\n",
    "        \n",
    "        # 토큰값 표시\n",
    "        plt.subplot(211)\n",
    "        plt.bar(range(tokens_to_show), first_doc_tokens[:tokens_to_show], color='skyblue')\n",
    "        plt.xlabel('토큰 인덱스')\n",
    "        plt.ylabel('토큰 ID')\n",
    "        plt.title('문서의 처음 몇 개 토큰')\n",
    "        \n",
    "        # 슬라이딩 윈도우 시각화\n",
    "        plt.subplot(212)\n",
    "        \n",
    "        windows = []\n",
    "        for i in range(0, min(tokens_to_show, len(first_doc_tokens) - max_length), stride):\n",
    "            window = np.zeros(tokens_to_show)\n",
    "            window[i:i+max_length] = 1\n",
    "            windows.append(window)\n",
    "        \n",
    "        if windows:\n",
    "            windows_array = np.array(windows)\n",
    "            plt.imshow(windows_array, aspect='auto', cmap='Greens')\n",
    "            plt.xlabel('토큰 인덱스')\n",
    "            plt.ylabel('윈도우 번호')\n",
    "            plt.title('슬라이딩 윈도우 시각화')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, '슬라이딩 윈도우를 생성할 수 없습니다.', \n",
    "                     horizontalalignment='center', verticalalignment='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 처리 결과 시각화 실행\n",
    "visualize_dataset_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1db824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db3870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩 및 데이터로더 생성 함수\n",
    "def create_dataloader(batch_size=4, max_length=1024, stride=512, shuffle=True, num_workers=2, column_name='0'):\n",
    "    \"\"\"\n",
    "    스트리밍 방식으로 데이터를 처리하는 데이터로더를 생성합니다.\n",
    "    \"\"\"\n",
    "    print(\"데이터셋 로딩 중...\")\n",
    "    \n",
    "    # 데이터셋 로드\n",
    "    ds = load_dataset(\"maywell/korean_textbooks\", data_files=\"mmlu_high_school_statistics/train-00000-of-00001.parquet\")\n",
    "    ds_train = ds['train']\n",
    "    \n",
    "    print(f\"데이터셋 로드 완료: {len(ds_train)} 문서\")\n",
    "    \n",
    "    # CustomDataset 생성\n",
    "    print(\"데이터셋 객체 생성 중...\")\n",
    "    dataset = CustomDataset(\n",
    "        ds_train,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        column_name=column_name\n",
    "    )\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    print(\"데이터로더 생성 중...\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers if torch.cuda.is_available() else 0,  # CPU 전용 모드에서는 0으로 설정\n",
    "        pin_memory=torch.cuda.is_available()  # GPU 사용 시 메모리 고정\n",
    "    )\n",
    "    \n",
    "    print(f\"데이터로더 생성 완료: batch_size={batch_size}, max_length={max_length}\")\n",
    "    return dataloader, dataset.tokenizer\n",
    "\n",
    "\n",
    "# 데이터로더 테스트 및 시각화 함수\n",
    "def test_dataloader(dataloader, num_batches=3):\n",
    "    \"\"\"\n",
    "    데이터로더를 테스트하고 첫 몇 개 배치의 정보를 출력합니다.\n",
    "    \"\"\"\n",
    "    print(\"데이터로더 테스트 중...\")\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    \n",
    "    for i, (input_ids, target_ids) in enumerate(tqdm(dataloader, total=num_batches)):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        print(f\"배치 {i+1}: 입력 형태: {input_ids.shape}, 타겟 형태: {target_ids.shape}\")\n",
    "        \n",
    "        # 첫 번째 배치의 샘플 토큰 확인\n",
    "        if i == 0:\n",
    "            # 첫 번째 시퀀스의 처음 10개 토큰\n",
    "            first_tokens = input_ids[0][:10].tolist()\n",
    "            first_target_tokens = target_ids[0][:10].tolist()\n",
    "            \n",
    "            print(f\"샘플 입력 토큰: {first_tokens}\")\n",
    "            print(f\"샘플 타겟 토큰: {first_target_tokens}\")\n",
    "            \n",
    "            # 증분 관계 확인 (자동회귀 패턴)\n",
    "            print(\"\\n자동회귀 패턴 확인:\")\n",
    "            for j in range(min(5, len(first_tokens))):\n",
    "                if j+1 < len(first_tokens):\n",
    "                    print(f\"입력[{j+1}] = {input_ids[0][j+1]} == 타겟[{j}] = {target_ids[0][j]}: {input_ids[0][j+1] == target_ids[0][j]}\")\n",
    "            \n",
    "            # 샘플 텍스트 디코딩\n",
    "            try:\n",
    "                sample_text = tokenizer.decode(input_ids[0][:30].tolist())\n",
    "                print(f\"\\n샘플 텍스트 (처음 30개 토큰): {sample_text}\")\n",
    "            except Exception as e:\n",
    "                print(f\"텍스트 디코딩 실패: {e}\")\n",
    "\n",
    "\n",
    "# 메인 실행 코드 - 노트북 환경에서 바로 실행 가능\n",
    "if __name__ == \"__main__\":\n",
    "    # 이 코드는 Jupyter Notebook에서 직접 실행하거나\n",
    "    # 아래와 같이 셀에서 따로 불러올 수 있습니다.\n",
    "    \n",
    "    # CUDA 사용 가능 여부 확인\n",
    "    print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # 하드웨어에 맞게 설정 조정\n",
    "    batch_size = 4        # 8GB VRAM에 맞게 조정\n",
    "    max_length = 1024     # 8GB VRAM에 맞게 조정\n",
    "    stride = 512          # 절반 오버랩\n",
    "    num_workers = 2       # 16GB RAM에 맞게 조정\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    loader, tokenizer = create_dataloader(\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # 데이터로더 테스트\n",
    "    test_dataloader(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9479239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시각화 함수 - 배치 내 토큰 분포 확인\n",
    "def visualize_batch_tokens(dataloader, batch_idx=0):\n",
    "    \"\"\"\n",
    "    배치 내 토큰 분포를 시각화합니다.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    \n",
    "    # 배치 가져오기\n",
    "    for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "        if i == batch_idx:\n",
    "            batch = input_ids\n",
    "            break\n",
    "    \n",
    "    # 배치를 numpy 배열로 변환\n",
    "    batch_np = batch.numpy()\n",
    "    \n",
    "    # 모든 토큰을 하나의 리스트로 평탄화\n",
    "    all_tokens = batch_np.flatten()\n",
    "    \n",
    "    # 토큰 빈도 계산\n",
    "    token_counts = Counter(all_tokens)\n",
    "    most_common = token_counts.most_common(20)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 상위 20개 토큰 빈도\n",
    "    tokens, counts = zip(*most_common)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(tokens)), counts)\n",
    "    plt.xticks(range(len(tokens)), [str(t) for t in tokens], rotation=45)\n",
    "    plt.title('상위 20개 토큰 빈도')\n",
    "    plt.xlabel('토큰 ID')\n",
    "    plt.ylabel('빈도')\n",
    "    \n",
    "    # 토큰 히스토그램\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(all_tokens, bins=50)\n",
    "    plt.title('토큰 분포 히스토그램')\n",
    "    plt.xlabel('토큰 ID')\n",
    "    plt.ylabel('빈도')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 토큰 디코딩 시도\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    for token, count in most_common[:10]:\n",
    "        try:\n",
    "            decoded = tokenizer.decode([int(token)])\n",
    "            print(f\"토큰 ID {token}: '{decoded}' (빈도: {count})\")\n",
    "        except:\n",
    "            print(f\"토큰 ID {token}: 디코딩 불가 (빈도: {count})\")\n",
    "\n",
    "\n",
    "# 자동회귀 패턴 시각적 확인\n",
    "def check_autoregressive_pattern(dataloader):\n",
    "    \"\"\"\n",
    "    데이터로더의 자동회귀 패턴을 확인합니다.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # 첫 번째 배치 가져오기\n",
    "    for input_ids, target_ids in dataloader:\n",
    "        break\n",
    "    \n",
    "    # 첫 번째 샘플의 처음 20개 토큰\n",
    "    input_sample = input_ids[0][:20].numpy()\n",
    "    target_sample = target_ids[0][:20].numpy()\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(input_sample, 'b-', label='입력 토큰')\n",
    "    plt.plot(target_sample, 'r-', label='타겟 토큰')\n",
    "    plt.legend()\n",
    "    plt.title('입력과 타겟의 자동회귀 패턴')\n",
    "    plt.xlabel('위치')\n",
    "    plt.ylabel('토큰 ID')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 오프셋 확인\n",
    "    offset_check = input_sample[1:] == target_sample[:-1]\n",
    "    all_match = offset_check.all()\n",
    "    \n",
    "    plt.figtext(0.5, 0.01, f\"자동회귀 패턴 일치: {all_match}\", \n",
    "                ha=\"center\", fontsize=12, \n",
    "                bbox={\"facecolor\":\"orange\" if not all_match else \"green\", \n",
    "                      \"alpha\":0.5, \"pad\":5})\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 상세 비교\n",
    "    print(\"\\n입력과 타겟 토큰 비교:\")\n",
    "    for i in range(len(input_sample)-1):\n",
    "        match = input_sample[i+1] == target_sample[i]\n",
    "        print(f\"입력[{i+1}]={input_sample[i+1]} vs 타겟[{i}]={target_sample[i]}: {'✓' if match else '✗'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm",
   "language": "python",
   "name": "slm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
